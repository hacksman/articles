
### 1. 反爬虫方法归纳
* 验证码、校验网上免费IP池程序、IP封禁各大云服务器、Session校验、封IP、cookies校验、User-Agent校验、Js加密生成动态token、JS生成图片码动态转文字、设置最多页数访问、403错误截断爬虫，脏数据

### 2. 需补充说明的方法案例总结
##### Session校验，Cookies校验
* + 相关案例：小程序CMB
	+ 反爬优势：通过生成session，监控某用户的访问访问情况，达到阙值清除session
	+ 具体细节：
	+  1. 两小时内请求数达到阙值，跳验证码校验，成功可允许继续访问，失败封禁IP
		2. 验证码出现时，两小时请求数阙值减半，以此类推，直至请求书阙值减为零，彻底封禁IP
##### Js动态token
*  + 相关案例：[华住价格数据]()、[合肥政府网站][2]
	+ 反爬优势：通过Javascript加密核心数据，增加爬虫成本，另外可定期更新js规则，增加大规模爬虫的成本
	+ 具体细节：
	+  1. 参考华住网站，将价格数据封装在js的一个图片中，每次访问该网站，图片都不一样，根据图片上数字的索引号对应为真实的价格数据
		2. 合肥政府网站通过对应的iptstamp对应的值，以这个唯一值，通过算法动态生成请求的url，每次访问必须通过最外层的链接逐层递进访问连接，增加爬虫成本
##### 403错误截断爬虫
* + 相关案例：[58二手房][3]
	+ 反爬优势：截断爬虫关键位置，让爬虫不能一次性爬取完成
	+ 具体细节：
	+ 1. 对page或者区域等二级页面设置3分钟内，访问次数校验，若超过阙值，返回403错误，封禁该用户访问该页面5分钟

### 3. 优秀参考资料：
* [神箭手爬虫博客][4]
* [知乎如何应对网站反爬虫策略…][5]
* [携程研发经理崔广宇关于爬虫和反爬虫技术分享][6]

### 4. 大牛的想法参考：
> 作者：知乎xlzd
> 
> 一个比较常见的反爬虫策略是基于访问数量，爬虫的访问总数会远高于人类，设定一个阈值，超过阈值的就是爬虫。常见使用这样处理方式的网站有 58 同城等，在访问 58 同城较快时，会弹出一个验证码。然而只要有规律的 sleep，就可以轻松绕过这条限制。
> 
> 这种处理方案的升级版是找到人与爬虫访问特征的不一致究竟在哪里。爬虫与人类在访问特征上最大的不一样在于，人不会长时间持续访问一个网站，正常人类在天级的时间周期里访问一个网站的总次数 y 大致满足——y = k*x^{a}  (0\< a \< 1, k \> 0)*
> 
> 我们不关心 k、a 的值具体是什么，但是比较明显的是，一个正常用户访问会在较短时间里完成某一时间周期的总请求数的绝大部分。映射到总用户上，确定的一段时间里，正常用户访问的总页数会在某个量级时开始骤减。
> 
> 而爬虫访问一个网站的总次数 y 与某个时间周期的关系则大致为——y = k*x(k \> 0)*
> 
> 其实就是，爬虫的访问数量会随着时间增长而线性增长。于是，根据这样的特点，可以参考人类社会的个人所得税制度或者阶梯电价制度，对于一个较短周期设置比较宽的阈值，而随着时间长度的增加而逐步收紧阈值。当然，具体的阈值设置为多少合适，要根据特定网站的日志分析之后得出具体数据。

[2]:	http://real.hffd.gov.cn/
[3]:	http://sz.58.com/ershoufang/?PGTID=0d300000-0000-0515-2481-904fac447c74&ClickID=1 "58"
[4]:	http://www.shenjian.io/blog/
[5]:	https://www.zhihu.com/question/28168585
[6]:	https://segmentfault.com/a/1190000005840672