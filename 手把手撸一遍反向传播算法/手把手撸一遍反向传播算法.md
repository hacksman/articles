
失败是成功之母，机器学习是你爹。
————奥图洛夫斯基·张小鸡

每一个认真生活的人，都应该认真学一下机器学习。骄傲者从中获得谦卑，浮躁者从中获得平静，就犹如体验过波涛风浪或生死洗礼一样。从此，便真真地知道了：自己是个弱鸡。

哔哔完了，进入正题，在此之前，希望你已经对基础的神经网络有一个了解，而反向传播算法，其实是一个寻求最优解的过程。包括我在内的很多人，一开始都认为其中的原理很难，但其实不是，反向传播算法之所以被广泛应用，除了有效性外，私以为还得益于它的容易理解。看过《土拨鼠之日》的人，一定希望拥有和男主一样的奇特经历。因为它的奇妙之处，是可以一次次地返回去实验，如果失败了，则得到反馈，再优化。相应的，反向传播算法也是如此，一次次的返回去重来，得到最优解。

下面咱们从从最简单三层神经网络开始，即输入层，隐含层和输出层。输入层包含两个神经元i1, i2，隐含层包含两个神经元h1, h2，输出层同样是两个神经元o1, o2，输入层和隐含层的偏置项分别为b1, b2，激活函数默认为sigmod函数。

现在我们对它们附上初始值，如下图所示：

其中输入数据：


还记得我们的最终目标吧？给出初始值i1,i2，求出使得离目标输出最接近的系数值，即（wi, bi）

一步步的推导过程我再次不赘述，可以参考下原文或胡晓曼大佬的文章，传送门分别戳这里，我说一下具体的代码实现部分的一些细节。Matt Mazur大佬清洗易读，而且分类特别赞，我们回顾一下上面的结构，发现整个神经网络系统就分为两个部分：神经元和连接神经元的线（即权重和偏置项），分工也很清晰明了。神经元负责的有：
向前传播：加权和，激活函数，输出函数，误差
反向传播：误差偏导，输出函数到激活函数的偏导，激活层到加权和的偏导

它们之间的连线，也就是网络层，负责对神经元的操控，它只需要负责向前传播和得到输出的结果。有了如上的结构，便可以来搭建整个网络了，黑喂狗


